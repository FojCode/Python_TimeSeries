# autocovariance function 
import numpy as np

def autocovariance(data, max_lag=None):
  """
  This function calculates the autocovariance of a time series data for a given maximum lag.

  Args:
      data: A numpy array containing the time series data.
      max_lag (int, optional): The maximum lag for which to compute the autocovariance. 
          Defaults to None (calculates autocovariance for all lags).

  Returns:
      A numpy array containing the autocovariance values for each lag.
  """

  n = len(data)
  mean = np.mean(data)
  autocov = np.zeros(max_lag or n)
for lag in range(max_lag or n):
    if lag == 0:
      autocov[lag] = np.var(data)  # Autocovariance at lag 0 is the variance
    else:
      autocov[lag] = np.sum((data[:-lag] - mean) * (data[lag:] - mean)) / (n - lag)

  return autocov

# Example usage
data = np.random.rand(100)  # Generate some random data
autocov = autocovariance(data, max_lag=20)

print(autocov)  # This will print the autocovariance values for lags 0 to 19

# autocorrelation function of a time series 
import numpy as np

def autocorrelation(data, max_lag=None):
  """
  This function calculates the autocorrelation of a time series data for a given maximum lag.

  Args:
      data: A numpy array containing the time series data.
      max_lag (int, optional): The maximum lag for which to compute the autocorrelation. 
          Defaults to None (calculates autocorrelation for all lags).

  Returns:
      A numpy array containing the autocorrelation values for each lag.
  """

  n = len(data)
  mean = np.mean(data)
  autocorr = np.zeros(max_lag or n)

  for lag in range(max_lag or n):
    if lag == 0:
      autocorr[lag] = 1.0  # Autocorrelation at lag 0 is always 1
    else:
      autocorr[lag] = np.sum((data[:-lag] - mean) * (data[lag:] - mean)) / (n - lag) / np.var(data)

  return autocorr

# Example usage
data = np.random.rand(100)  # Generate some random data
autocorr = autocorrelation(data, max_lag=20)

print(autocorr)  # This will print the autocorrelation values for lags 0 to 19


# autocorrelation for arma(1,1)
import numpy as np

def arma_autocorrelation(phi, theta):
  """
  Calculates the theoretical autocorrelation function for an ARMA(1,1) process.

  Args:
      phi (float): AR coefficient.
      theta (float): MA coefficient.

  Returns:
      A list containing the theoretical autocorrelation values for different lags.
  """

  autocorr = []
  autocorr.append(1.0)  # Autocorrelation at lag 0 is always 1

  for k in range(1, n_lags):
    rho = phi**k + theta * phi**(k-1)
    autocorr.append(rho)

  return autocorr
# Example usage
phi = 0.8
theta = -0.5
n_lags = 20  # Number of lags for autocorrelation

theoretical_autocorr = arma_autocorrelation(phi, theta)

# Print the first few theoretical autocorrelation values
print("Theoretical Autocorrelation:")
for i in range(min(n_lags, 10)):
  print(f"Lag {i+1}: {theoretical_autocorr[i]:.4f}")

# partial autocorrelation function 
import numpy as np
from statsmodels.tsa.stattools import pacf

def partial_autocorrelation(data, max_lag=None):
  """
  Calculates the partial autocorrelation (PACF) of a time series data for a given maximum lag.

  Args:
      data: A numpy array containing the time series data.
      max_lag (int, optional): The maximum lag for which to compute the PACF. 
          Defaults to None (calculates PACF for all lags).

  Returns:
      A numpy array containing the PACF values for each lag.
  """

  pacf_values = pacf(data, nlags=max_lag, method='ols')
  return pacf_values

# Example usage
data = np.random.rand(100)  # Generate some random data
pacf = partial_autocorrelation(data, max_lag=20)

print(pacf)  # This will print the PACF values for lags 0 to 19

# partial autocorrelation function for ma(1)

import numpy as np
from statsmodels.tsa.stattools import pacf

def pacf_for_ma1(data, max_lag=None):
  """
  This function calculates the PACF of a time series data, emphasizing the theoretical result for MA(1).

  Args:
      data: A numpy array containing the time series data.
      max_lag (int, optional): The maximum lag for which to compute the PACF. 
          Defaults to None (calculates PACF for all lags).

  Returns:
      A numpy array containing the PACF values for each lag.
  """

  pacf_values = pacf(data, nlags=max_lag, method='ols')

  # Note for MA(1): Theoretical PACF is 0 for lags greater than 1
  if isinstance(data, np.ndarray) and len(data) > 1:
    pacf_values[2:] = 0  # Set PACF to 0 for lags > 1 (assuming MA(1))
  
  return pacf_values

# Example usage
data = np.random.rand(100)  # Generate some random data
pacf = pacf_for_ma1(data, max_lag=20)

print(pacf)  # This will print the PACF values for lags 0 to 19

# spectral density function 
import numpy as np
from scipy.signal import welch

def psd(data, fs=1.0, window="hann", nperseg=None):
  """
  This function estimates the power spectral density (PSD) of a time series.

  Args:
      data: A list of numerical values representing the time series.
      fs: The sampling frequency of the time series (default 1.0).
      window: The window function to use ("hann", "hamming", "blackman", etc., default "hann").
      nperseg: The length of each window segment (default None, uses Welch's method).

  Returns:
      A tuple containing:
          - Frequencies (f) in Hz
          - Power spectral density (Pxx) in units^2 / Hz
  """
  f, Pxx = welch(data, fs=fs, window=window, nperseg=nperseg)
  return f, Pxx

# Example usage
data = np.sin(2*np.pi*0.1*np.arange(100)) + np.random.rand(100)  # Sample data with sine wave and noise
fs = 100  # Sampling frequency

f, Pxx = psd(data.copy(), fs)

# Plot results (using matplotlib)
import matplotlib.pyplot as plt

plt.semilogy(f, Pxx)
plt.xlabel("Frequency (Hz)")
plt.ylabel("Power spectral density (units^2 / Hz)")
plt.title("Power Spectral Density of the Time Series")
plt.grid(True)
plt.show()


# spectral density function for a arma(p,q)
import numpy as np
from scipy.signal import welch
from statsmodels.tsa.arma_model import ARMA

def arma_simulated_psd(ar_params, ma_params, n_sims, fs=1.0, window="hann", nperseg=None):
  """
  This function simulates data from an ARMA(p,q) model and estimates its PSD.

  Args:
      ar_params: A list of AR coefficients.
      ma_params: A list of MA coefficients (can be empty for pure AR).
      n_sims: The number of data points to simulate.
      fs: The sampling frequency (default 1.0).
      window: The window function to use ("hann", "hamming", "blackman", etc., default "hann").
      nperseg: The length of each window segment (default None, uses Welch's method).

  Returns:
      A tuple containing:
          - Frequencies (f) in Hz
          - Power spectral density (Pxx) in units^2 / Hz
  """
  model = ARMA(np.zeros(n_sims), order=(len(ar_params), len(ma_params)))
  model.arparams = ar_params
  model.maparams = ma_params
  simulated_data = model.simulate(nsims=n_sims)

  f, Pxx = welch(simulated_data, fs=fs, window=window, nperseg=nperseg)
  return f, Pxx

# Example usage (assuming you have estimated ARMA parameters)
ar_params = [0.8]  # Example AR parameters
ma_params = []  # Example MA parameters (pure AR here)
n_sims = 1000  # Number of data points to simulate
fs = 100  # Sampling frequency

f, Pxx = arma_simulated_psd(ar_params, ma_params, n_sims, fs)

# Plot results (using matplotlib)
import matplotlib.pyplot as plt

plt.semilogy(f, Pxx)
plt.xlabel("Frequency (Hz)")
plt.ylabel("Power spectral density (units^2 / Hz)")
plt.title("Estimated PSD of ARMA(1,0) Model")
plt.grid(True)
plt.show()


# inversion formula of a time series 
def invert_differenced_series(data):
  """
  This function inverts a differenced time series using cumulative summation.

  Args:
      data: A list of numerical values representing the differenced time series.

  Returns:
      The original time series.
  """
  inverted_data = [data[0]]
  for value in data[1:]:
    inverted_data.append(inverted_data[-1] + value)
  return inverted_data

# Example usage
differenced_data = [10, 2, 5, -1]  # Sample differenced series
original_data = invert_differenced_series(differenced_data)
print(original_data)  # Output: [10, 12, 17, 16]


# linear filters of a time series 
import numpy as np
from scipy.signal import filtfilt, butter, iirfilter

def moving_average_filter(data, window_size):
  """
  This function applies a moving average filter to the time series.

  Args:
      data: A list of numerical values representing the time series.
      window_size: The size of the moving average window (must be odd).

  Returns:
      The filtered time series.
  """
  if window_size % 2 == 0:
    raise ValueError("Window size must be odd for moving average filter.")
  weights = np.ones(window_size) / window_size
  return filtfilt(weights, 1, data)  # filtfilt for causal filtering

def butterworth_filter(data, cutoff_freq, fs, filter_type="lowpass"):
  """
  This function applies a Butterworth filter to the time series.

  Args:
      data: A list of numerical values representing the time series.
      cutoff_freq: The cutoff frequency for the filter (normalized by sampling rate).
      fs: The sampling frequency of the time series.
      filter_type: The type of filter ("lowpass", "highpass", "bandpass", or "bandstop").

  Returns:
      The filtered time series.
  """
  nyq = fs / 2  # Nyquist frequency
  normalized_cutoff = cutoff_freq / nyq
  order, wn = butter(N=None, Wn=normalized_cutoff, btype=filter_type)
  b, a = butter(order, wn)
  return filtfilt(b, a, data)

def iirfilter_design(data, f1, f2, fs, filter_type="bandpass"):
  """
  This function applies a custom IIR filter using the iirfilter function.

  Args:
      data: A list of numerical values representing the time series.
      f1: Lower cutoff frequency for bandpass or bandstop filter (optional).
      f2: Upper cutoff frequency for bandpass or bandstop filter (optional).
      fs: The sampling frequency of the time series.
      filter_type: The type of filter ("lowpass", "highpass", "bandpass", or "bandstop").

  Returns:
      The filtered time series.
  """
  nyq = fs / 2
  if filter_type in ["bandpass", "bandstop"]:
    f1_normalized = f1 / nyq
    f2_normalized = f2 / nyq
    b, a = iirfilter(filter_type, f1=f1_normalized, f2=f2_normalized)
  else:
    cutoff_freq = f1 / nyq  # Assuming f1 is the cutoff frequency for other filter types
    b, a = iirfilter(filter_type, cutoff=cutoff_freq)
  return filtfilt(b, a, data)

# Example usage
data = np.sin(2*np.pi*0.1*np.arange(100)) + np.random.rand(100)  # Sample data with sine wave and noise
fs = 100  # Sampling frequency

# Moving average filter
filtered_data_ma = moving_average_filter(data.copy(), window_size=5)

# Butterworth lowpass filter
cutoff_freq = 0.2  # Normalized cutoff frequency (0 to 0.5)
filtered_data_butter = butterworth_filter(data.copy(), cutoff_freq, fs)

# Custom IIR bandpass filter
f1 = 0.1  # Lower cutoff frequency
f2 = 0.3  # Upper cutoff frequency
filtered_data_iir = iirfilter_design(data.copy(), f1, f2, fs)

# Plot results (using matplotlib)
import matplotlib.pyplot as plt

plt.plot(data, label="Original data")
plt.plot(filtered_data_ma, label="Moving average")
plt.plot(filtered_data_butter, label="Butterworth lowpass")
plt.plot(filtered_data_iir, label="Custom IIR bandpass")
plt.legend()
plt.show()


# Ljung and Box portmanteau test of residuals 
import statsmodels.api as sm

def arma_ljungbox_test(data, p, q, lags=None):
  """
  This function fits an ARMA(p,q) model to the data and performs the Ljung-Box test on the residuals.

  Args:
      data: A list of numerical values representing the time series.
      p: The AR order of the model.
      q: The MA order of the model.
      lags: The number of lags to test (default None, uses a rule based on data length).

  Returns:
      A tuple containing:
          - The Ljung-Box test statistic
          - The p-value for the Ljung-Box test
  """
  model = sm.tsa.ARMA(data, (p, q)).fit(disp=-1)
  lb_results = sm.stats.acorr_ljungbox(model.resid, lags=lags)
  return lb_results.lb_stat[0], lb_results.lb_pvalue[0]

# Example usage
data = [....]  # Replace with your actual time series data
p = 1  # AR order (adjust as needed)
q = 1  # MA order (adjust as needed)
lags = 10  # Number of lags to test (optional)

lb_stat, lb_pvalue = arma_ljungbox_test(data, p, q, lags)

print(f"Ljung-Box test statistic: {lb_stat}")
print(f"Ljung-Box p-value: {lb_pvalue}")

# For Box-Pierce test, use boxpierce=True in acorr_ljungbox function


# Turning point test 
def turning_points(data):
  """
  This function calculates the number of turning points in a time series.

  Args:
      data: A list of numerical values representing the time series.

  Returns:
      The number of turning points in the data.
  """
  turning_points = 0
  for i in range(2, len(data)):
    if (data[i-1] < data[i] and data[i] > data[i+1]) or (data[i-1] > data[i] and data[i] < data[i+1]):
      turning_points += 1
  return turning_points

def turning_point_test(data, alpha=0.05):
  """
  This function performs the turning point test on a time series.

  Args:
      data: A list of numerical values representing the time series.
      alpha: The significance level for the test (default 0.05).

  Returns:
      A tuple containing:
          - The test statistic (number of turning points)
          - The p-value
          - True if the null hypothesis is rejected (data not random)
  """
  n = len(data)
  expected_turning_points = (n - 2) * 2 / 3
  variance = 8 * n / 45
  test_statistic = turning_points(data)
  z_score = (test_statistic - expected_turning_points) / np.sqrt(variance)
  p_value = 1 - (1 - np.erf(z_score / np.sqrt(2))) / 2
  reject_null = p_value < alpha
  return test_statistic, p_value, reject_null

# Example usage
import numpy as np

# Sample time series data
data = np.random.rand(100)

# Perform the turning point test
test_statistic, p_value, reject_null = turning_point_test(data)

print(f"Test statistic: {test_statistic}")
print(f"p-value: {p_value}")

if reject_null:
  print("Reject null hypothesis: The data is not random at a significance level of {alpha}.")
else:
  print("Fail to reject null hypothesis: The data may be random.")
